import requests
from typing import Optional

class LLMAPI:
    def __init__(self, api_key: str, model: str = "gemini"):
        """
        Initialize the LLMAPI class.

        Args:
            api_key (str): The API key to use for Gemini LLM API requests.
            model (str, optional): The model name to use for generation. Defaults to "gemini".
        """
        self.api_key = api_key
        self.model = model
        self.call_api_functions = {
            "gemini": self._call_gemini_api,
            "custom": self._call_custom_api,
        }
        if model not in self.call_api_functions:
            raise ValueError(
                f"Model name '{model}' is not supported. Supported models are {list(self.call_api_functions.keys())}.")

    def _call_gemini_api(self, context: str, api_key: Optional[str] = None) -> Optional[str]:
        """
        Makes a request to the Gemini LLM API with the given context.

        Args:
            context (str): The context string that guides the LLM response.

        Returns:
            str: The response text generated by the Gemini LLM, in JSON format.
        """
        headers = {"Content-Type": "application/json"}
        payload = {"contents": [{"parts": [{"text": context}]}]}
        response = requests.post(
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={self.api_key}",
            headers=headers,
            json=payload,
        )
        if response.status_code == 200:
            response_json = response.json()
            try:
                # Extract the generated response text
                model_output = response_json["candidates"][0]["content"]["parts"][0][
                    "text"
                ]
                return model_output
            except (KeyError, IndexError):
                print("Error: Gemini LLM response is not in the expected format.")
                return None
        else:
            print(f"Error: {response.status_code} - {response.text}")
            return None

    def _call_custom_api(self, context: str) -> str:
        """Makes a request to the custom API with the given context.

        Args:
            context (str): The context string that guides the LLM response.

        Returns:
            str: The response text generated by the custom API, in JSON format.
        """
        return "Custom API response"

    def generate_response(self, context: str, api_key: Optional[str] = None, model: Optional[str] = None) -> Optional[str]:
        """
        Calls the internal _call_gemini_api function to generate a response from the Gemini LLM.

        Args:
            context (str): The context string that guides the LLM response.
            api_key (str, optional): The API key to use for the request. Defaults to None.
            model (str, optional): The model name to use for generation. Defaults to None.

        Returns:
            str: The response text generated by the Gemini LLM, in JSON format.
        """
        return self.call_api_functions[model if model else self.model](context, api_key or self.api_key)

